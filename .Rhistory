# WARNING: Do not modify the codes in here.
# Run this code before moving to the next one
library(PerformanceAnalytics)
library(xts)
library(lubridate)
library(tidyverse)
library(dplyr)
library(caret)
library(e1071)
library(class)
library(ggplot2)
library(Metrics)
my_factors <- read.csv("Data_RLab3.csv") # call the data
my_factors$Date <- mdy(my_factors$Date) # declare the date variable
my_factors_sorted<- my_factors[order(my_factors$Date),] # sort by date
All_data <- xts(my_factors_sorted[,-1],order.by = my_factors_sorted[,1],)
All_data$Y<-All_data$Brk_ret-All_data$RF  # target variable
Full_data<-as.data.frame(All_data) # convert to data frame
Fulldata = subset(Full_data, select = -c(RF,Brk_ret,Brk_exret,Subperiod, Mkt))# drop redundant ones
Fulldata<-Fulldata%>%
rename(MRP=Mkt_rf, MOM=Mom)
first400<-Fulldata[1:400,]  # use the first 400 as training and validation set
testset<-Fulldata[401:500,]  # last 100 for the test set
set.seed(5410)   # use this seed
# shuffle the index for the testing data
shuffle<-sample(nrow(first400), 0.25*nrow(first400))
# Get the training data in training set
trainingset<-first400[-shuffle,]
# Get the validation set in trainingf  data
validationset<-first400[shuffle,]
# HINT: use step function and choose either criterion = "AIC" and criterion = "BIC"
# Enter your code below
minM <- lm(Y~ 1, data=trainingset)
maxM<-lm(Y~. , data=trainingset)
#model_forward <- step(lm(Y ~ ., data = trainingset), direction = "forward", trace = FALSE, k = log(nrow(trainingset))) # I dont understand how this line of code is the same as the following line of code.
model_forward <-stats::step(minM ,direction= "forward", scope=formula(maxM), criterion = "AIC") #is using the stepwise regression method to build a linear                          regression model.
summary(model_forward)
# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.
#Model_backward <-stats::step(maxM,direction="backward", scope=formula(maxM),criterion ="BIC")
#summary(Model_backward)
# HINT: use step function and choose either criterion = "AIC" and criterion = "BIC"
# Enter your code below
# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.
model_backward <-stats::step(maxM,direction="backward", scope=formula(maxM),criterion ="BIC")
summary(model_backward)
# HINT 1: Use predict () function to get the predictions. RMSE formula: sqrt(mean((Actual-Fitted)^2))
# Fit model_forward on validationset and calculate RMSE
pred_forward <- predict(model_forward, newdata = validationset)
#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.
RMSE_forward <- sqrt(mean((pred_forward - validationset$Y)^2))
summary(pred_forward)
print(RMSE_forward)
# HINT 2: An easier way would be to use rmse() function in Metrics package
# Enter your code below
# Fit model_backward on validationset and calculate RMSE
pred_backward <- predict(model_backward, newdata = validationset)
#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.
RMSE_backward <- sqrt(mean((pred_backward - validationset$Y)^2))
summary(pred_backward)
print(RMSE_backward)
# HINT: Use preProcess=c('center', 'scale') to preprocess the data
#preProcess=c('center', 'scale')
# HINT: Use preProcess=c('center', 'scale') to preprocess the data
preproc <- list(preProcess = c('center', 'scale'))
# train knn model with first400 data
knn_model1 <- train(Y ~ MRP + SMB, data = first400, method = "knn", preProcess = c('center', 'scale'),
tuneGrid = expand.grid(k = seq(1, 50, by = 2)), trControl = trainControl(method = "cv", number = 10))
# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search
#tuneGrid = expand.grid(k = seq(1, 50, by = 2))
# HINT: Use trControl = trainControl(method = "CV", number = 10) for 10-fold cros validation
trControl <- trainControl(method = "cv", number = 10)
# HINT: knn_model1$results will produce the average results
optimal_k <- knn_model1$bestTune$k
RMSE <- knn_model1$results[knn_model1$results$k == optimal_k, "RMSE"]
RMSE
# Enter your code below
set.seed(2022)
# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search
# HINT: Use preProcess=c('center', 'scale') to preprocess the data
# HINT: Use trControl = trainControl(method = "CV", number = 10) for 10-fold cros validation
# HINT: knn_model5$bestTune will produce the average results
# Enter your code below
set.seed(2022)
# Define preprocessing method
preproc <- c('center', 'scale')
# Train KNN model for model5
knn_model5 <- train(Y ~ MRP + SMB + HML + MOM + BAB + QMJ, data = first400,
method = "knn", preProcess = preproc,
tuneGrid = expand.grid(k = seq(1, 50, by = 2)),
trControl = trainControl(method = "cv", number = 10))
# Find optimal k value
optimal_k <- knn_model5$bestTune$k
optimal_k
# HINT: use predict function
# HINT: you canuse rmse() function in Metrics package
# Enter your code below
# predict Y in testset using knn_model5
knn_model5_predict <- predict(knn_model5, newdata = testset)
# calculate RMSE value in testset
RMSE <- sqrt(mean((testset$Y - knn_model5_predict)^2))
# print RMSE value
RMSE
# HINT: use predict() function to get the predictions on testset
# HINT: you canuse rmse() function in Metrics package to calculate RMSE values
# Enter your code below
# Predictions for knn_model1
knn_model1_predict <- predict(knn_model1, newdata = testset)
rmse(testset$Y, knn_model1_predict)
# Predictions for knn_model5
knn_model5_predict <- predict(knn_model5, newdata = testset)
rmse(testset$Y, knn_model5_predict)
# Predictions for model_forward
model_forward_predict <- predict(model_forward, newdata = testset)
rmse(testset$Y,model_forward_predict )
# Predictions for model_backward
model_backward_predict <- predict(model_backward, newdata = testset)
rmse(testset$Y,model_backward_predict )
#library(Metrics)
RMSE_knn1 <- rmse(predict(knn_model1, newdata = testset), testset$Y)
RMSE_knn5 <- rmse(knn_model5_predict, testset$Y)
RMSE_backward <- rmse(model_backward_predict, testset$Y )
RMSE_forward <- rmse(model_forward_predict, testset$Y )
# print(paste("The value of my object is", my_object))
print(paste("The value of RMSE_knn1 = ", RMSE_knn1))
print(paste("The value of RMSE_knn5 = ", RMSE_knn5))
print(paste("The value of RMSE_backward = ", RMSE_backward))
print(paste("The value of RMSE_forward = ", RMSE_forward))
# Run this code chunk without changing it
library(dplyr)
library(caret)
library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)
# Run this code chunk without changing it
creditdata<-read.csv("MP2_data_option1.csv", header = TRUE)
# Run this code chunk without changing it
library(dplyr)
library(caret)
library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)
# Run this code chunk without changing it
creditdata<-read.csv("MP2_data_option1.csv", header = TRUE)
# Run this code chunk without changing it
library(dplyr)
library(caret)
library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)
# Run this code chunk without changing it
creditdata<-read.csv("MP2_data_option1.csv", header = TRUE)
creditdata
# Insert your codes for Task 1A  below
# Drop rows from creditdata when Status variable takes the value of 0.
creditdata1 <- subset(creditdata, Status != 0)
creditdata1
# Drop rows from creditdata when Marital variable takes the value of 0.
creditdata2 <- subset(creditdata1, Marital != 0)
creditdata2
# Drop rows from creditdata when Job variable takes the value of 0.
creditdata3 <- subset(creditdata2, Job != 0)
creditdata3
# Drop rows from creditdata when Income, Assets, or Debt variable takes the value of 99999999.
creditdata4 <- subset(creditdata3, Income != 99999999 & Assets != 99999999 & Debt != 99999999)
creditdata4
# Declare the following variables as factor: Status, Home, Marital, Records, and Job
creditdata4$Status <- factor(creditdata4$Status)
creditdata4$Home <- factor(creditdata4$Home)
creditdata4$Marital <- factor(creditdata4$Marital)
creditdata4$Records <- factor(creditdata4$Records)
creditdata4$Job <- factor(creditdata4$Job)
# Label Status variable as "Good" and "Bad"
creditdata4$Status <- factor(creditdata4$Status, levels = c(1, 2), labels = c("Good", "Bad"))
creditdata4
# Insert your codes for Task 1B  below
set.seed(5410)
index <- createDataPartition(creditdata4$Status, p = 0.75, list = FALSE)
train_data <- creditdata4[index, ]
test_data <- creditdata4[-index, ]
train_data
test_data
# Insert your codes for Task 2A  below
model_tree <- rpart(Status ~ ., data = train_data, method = "class",
control = rpart.control(cp = 0.001, minbucket = 3, xval = 10))
plot(model_tree)
#Here is the code you can use to extract the variable importance measures from the model_tree object:
var_importance <- varImp(model_tree, scale = FALSE)
var_importance
# To extract the top three most important variables, you can use the following code:
top_vars <- sort(var_importance[, "Overall"], decreasing = TRUE)[1:3]
top_vars
# Insert your codes for Task 2B  below
# To use the model_tree object to predict Status labels in test_data and store them as predict_model_tree, you can use the predict() function in R with type="class".
predict_model_tree <- predict(model_tree, newdata = test_data, type = "class")
predict_model_tree
# Calculate accuracy and name it as accuracy_model_tree
actual_labels <- test_data$Status
accuracy_model_tree <- mean(predict_model_tree == actual_labels)
accuracy_model_tree
# Calculate precision and name it as precision_model_tree
conf_matrix <- confusionMatrix(predict_model_tree, actual_labels)
conf_matrix
precision_model_tree <- conf_matrix$byClass["Pos Pred Value"]
precision_model_tree
# Calculate sensitivity and name it as sensitivity_model_tree
conf_matrix <- confusionMatrix(predict_model_tree, actual_labels)
sensitivity_model_tree <- conf_matrix$byClass["Sensitivity"]
sensitivity_model_tree
# Calculate specificity and name it as specificity_model_tree
conf_matrix <- confusionMatrix(predict_model_tree, actual_labels)
specificity_model_tree <- conf_matrix$byClass["Specificity"]
specificity_model_tree
# Insert your codes for Task 2C  below
# In this project, search through mtry values 2,5,7,9,11, and 13 and use "gini" as the split rule (splitrule). For minimum node size, check values from 1 to 5 (min.node.size).
set.seed(5410) # set seed for reproducibility
tune_grid <- expand.grid(mtry = c(2, 5, 7, 9, 11, 13),
splitrule = "gini",
min.node.size = 1:5)
model_rf <- train(Status ~ ., data = train_data, method = "ranger",
tuneGrid = tune_grid,
trControl = trainControl(method = "cv", number = 10),
importance = "permutation")
#What is the highest accuracy measure in model_rf? Which specific parameters (mtry and min.node.size) give us the highest accuracy?
summary(model_rf)
# What is the highest accuracy measure in model_rf? Which specific parameters (mtry and min.node.size) give us the highest accuracy?
best_acc <- max(model_rf$results$Accuracy)
best_acc
best_params <- subset(model_rf$results, Accuracy == best_acc,
select = c(mtry, min.node.size))
best_params
# Insert your codes for Task 2D  below
#predict_model_rf <- predict(model_rf$finalModel, newdata = test_data, type = "class")
#predict_model_rf <- predict(model_rf$finalModel, data = test_data, type = "class")
predict_model_rf <- predict(model_rf, newdata=test_data, type = "raw")
summary(predict_model_rf)
actual_labels <- factor(test_data$Status, levels = levels(train_data$Status))
conf_matrix_rf <- confusionMatrix(predict_model_rf, actual_labels)
conf_matrix_rf
# Insert your codes for Task 2E  below
set.seed(5410) # set seed for reproducibility
tune_grid <- expand.grid(mtry = c(2, 3, 8, 11, 15, 19),
splitrule = "gini",
min.node.size = 1:7)
model_rf_best <- train(Status ~ ., data = train_data, method = "ranger",
tuneGrid = tune_grid,
trControl = trainControl(method = "oob", number = 7),
importance = "permutation")
#Continuing task 2E
predict_model_rf <- predict(model_rf, newdata=test_data, type = "raw")
summary(predict_model_rf)
actual_labels <- factor(test_data$Status, levels = levels(train_data$Status))
conf_matrix_rf <- confusionMatrix(predict_model_rf, actual_labels)
conf_matrix_rf
#This model is worse!
#Attempt #2 for Task 2e
set.seed(5410) # set seed for reproducibility
tune_grid <- expand.grid(mtry = c(1, 2, 4, 5, 7, 9),
splitrule = "gini",
min.node.size = 1:7)
model_rf <- train(Status ~ ., data = train_data, method = "ranger",
tuneGrid = tune_grid,
trControl = trainControl(method = "cv", number = 15),
importance = "permutation")
#Part of the 2nd attempt
predict_model_rf <- predict(model_rf, newdata=test_data, type = "raw")
summary(predict_model_rf)
actual_labels <- factor(test_data$Status, levels = levels(train_data$Status))
conf_matrix_rf <- confusionMatrix(predict_model_rf, actual_labels)
conf_matrix_rf
#This one is a bit better
# Insert your codes for Task 3A  below
set.seed(5410)
Logistic1 <- train(Status ~ Income + Price + Amount,
data=train_data,
method = "glm",
# tuneGrid = tune_grid,
trControl = trainControl(method = "cv", number = 10),
)
Logistic2 <- train(Status ~ Seniority + Home + Time + Age + Marital + Records + Job + Expenses + Income + Assets + Debt + Amount + Price,
data=train_data,
method = "glm",
trControl = trainControl(method = "cv", number = 10),
)
# Insert your codes for Task 3B  below
predict_Logistic1 <- predict(Logistic1, newdata=test_data, type="raw")
summary(predict_Logistic1)
predict_Logistic2 <- predict(Logistic2, newdata=test_data, type="raw")
summary(predict_Logistic2)
#Accuracy, Sensitivity, and Specificity
accuracylogistic1 <- mean(predict_Logistic1== actual_labels)
accuracylogistic1
accuracylogistic2 <- mean(predict_Logistic2== actual_labels)
accuracylogistic2
# Calculate precision and name it as precision_model_tree
confmatrixLog1 <- confusionMatrix(predict_Logistic1, actual_labels)
confmatrixLog1
confmatrixLog2 <- confusionMatrix(predict_Logistic2, actual_labels)
confmatrixLog2
precisionlog1 <- confmatrixLog1$byClass["Pos Pred Value"]
precisionlog1
precisionlog2 <- confmatrixLog2$byClass["Pos Pred Value"]
precisionlog2
# Calculate specificity and name it as specificity_model_tree
confmatlog1 <- confusionMatrix(predict_Logistic1, actual_labels)
specificity_log1 <- confmatlog1$byClass["Specificity"]
specificity_log1
confmatlog2 <- confusionMatrix(predict_Logistic2, actual_labels)
specificity_log2 <- confmatlog2$byClass["Specificity"]
specificity_log2
