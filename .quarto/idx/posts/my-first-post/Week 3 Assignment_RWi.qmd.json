{"title":"Week 3 Assignment","markdown":{"yaml":{"title":"Week 3 Assignment","format":{"html":{"embed-resources":true}},"editor":"visual"},"headingText":"Week 3 Assignment","containsRefs":false,"markdown":"\n\n\n## Return Prediction: Brief Explanation\n\nIn Lab3, we will use the stock return data. We have monthly excess return data on a diversified portfolio along with 5 factors that are shown in the literature to have an impact on stock returns. Nobel Laureate Eugene Fama and Kenneth French originally introduced the three factors model in their 1993 Journal of Financial Economics article. Then, 2 more factors were added to the original model, called the 5-factor model. In this lab assignment, your task is to apply model selection techniques, knn, and cross-validation methods to come up with a model that will be used to estimate stock returns in new data set and the test root mean squared error (RMSE).\n\n-   **TARGET VARIABLE:** Excess return on a diversified portfolio and it is captured as return on a portfolio - risk free rate (return on Long-term US Government Bond returns).\n\n-   **5-Factors:**\n\n    1\\. SIZE:Small-cap stocks tend to outperform large-cap stocks (Size is measured by stock price \\* shares outstanding)\n\n    2\\. VALUE: Cheaper stocks (Value stocks) tend to outperform expensive (Growth) stocks (Inexpensiveness: Book Value/Market Value, Book to Market ratio, B/M)\n\n    \\- Lower the B/M, expensive the stock (Growth stocks)\n\n    \\- Higher the B/M, cheap the stock (Value Stocks)\n\n    3\\. MOMENTUM: Winners outperform losers\n\n    4\\. RISK (BETA): Lower the beta of a stock, higher the return performance\n\n    5\\. QUALITY: Higher the profitability, higher the return performance\n\n## Data Dictionary\n\n-   We have 500 observations in the original data, called Full_data. Data spans from November 1976 till June 2018.\n\n-   We divided Full_data into two sets: first400 and testset. The first 400 monthly observations were kept for training and validation purposes. Monthly data from November 1976 till February 2010 were randomly divided into two groups. You can use the trainingset to train alternative models and validationset to check your model performance.\n\n-   **testset**: The last 100 monthly observations are kept as our testing data and it spans from March 2010 till June 2018.\n\n**Target Variable**\n\n**Y**: Excess return on a portfolio= Portolio return - risk free rate (return on US Government bonds)\n\n**Factors (Predictors)**\n\n1\\. SMB to capture size\n\n2\\. HML to capture Value\n\n3\\. MOM to capture Momentum\n\n4\\. BAB to capture Risk\n\n5\\. QMJ to capture Quality\n\n6\\. MRP: A measure of average market risk premium: measures as return on a value-weighted market portfolio - risk free rate.\n\nRun the following R chunk code before working on the questions.\n\n```{r, echo=FALSE}\n# WARNING: Do not modify the codes in here. \n# Run this code before moving to the next one\n\nlibrary(PerformanceAnalytics)\nlibrary(xts)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(e1071)\nlibrary(class)\nlibrary(ggplot2)\nlibrary(Metrics)\n\n\nmy_factors <- read.csv(\"Data_RLab3.csv\") # call the data\nmy_factors$Date <- mdy(my_factors$Date) # declare the date variable\nmy_factors_sorted<- my_factors[order(my_factors$Date),] # sort by date\nAll_data <- xts(my_factors_sorted[,-1],order.by = my_factors_sorted[,1],)\nAll_data$Y<-All_data$Brk_ret-All_data$RF  # target variable\n\nFull_data<-as.data.frame(All_data) # convert to data frame\nFulldata = subset(Full_data, select = -c(RF,Brk_ret,Brk_exret,Subperiod, Mkt))# drop redundant ones\nFulldata<-Fulldata%>%\n    rename(MRP=Mkt_rf, MOM=Mom)\n\nfirst400<-Fulldata[1:400,]  # use the first 400 as training and validation set\ntestset<-Fulldata[401:500,]  # last 100 for the test set\n\nset.seed(5410)   # use this seed\n# shuffle the index for the testing data\nshuffle<-sample(nrow(first400), 0.25*nrow(first400))\n # Get the training data in training set\ntrainingset<-first400[-shuffle,]\n# Get the validation set in trainingf  data\nvalidationset<-first400[shuffle,]\n\n\n```\n\n## PART I\n\nIn this part, you will be totally blind to **testset** (You can't use **testset** in part I).\n\n#### Part I: Question 1\n\nUse the step function in stats package and run a forward stepwise regression on **trainingset** and name your model **model_forward**. If we use AIC information criteria, which variables are selected based on model_forward?\n\n```{r, echo=TRUE}\n# HINT: use step function and choose either criterion = \"AIC\" and criterion = \"BIC\"\n# Enter your code below\n\nminM <- lm(Y~ 1, data=trainingset)\nmaxM<-lm(Y~. , data=trainingset)\n\n\n#model_forward <- step(lm(Y ~ ., data = trainingset), direction = \"forward\", trace = FALSE, k = log(nrow(trainingset))) # I dont understand how this line of code is the same as the following line of code.\n\n\nmodel_forward <-stats::step(minM ,direction= \"forward\", scope=formula(maxM), criterion = \"AIC\") #is using the stepwise regression method to build a linear                          regression model.   \n\nsummary(model_forward)\n\n```\n\n```{r}\n# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.\n\n#Model_backward <-stats::step(maxM,direction=\"backward\", scope=formula(maxM),criterion =\"BIC\")\n\n#summary(Model_backward)\n```\n\n#### Part I: Question 2\n\nUse the step function in stats package and run a backward stepwise regression on **trainingset** and name your model **model_backward**. If we use **BIC** information criteria, which variables are selected based on **model_backward**?\n\n```{r, echo=TRUE}\n# HINT: use step function and choose either criterion = \"AIC\" and criterion = \"BIC\"\n# Enter your code below\n\n# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.\n\nmodel_backward <-stats::step(maxM,direction=\"backward\", scope=formula(maxM),criterion =\"BIC\")\n\nsummary(model_backward)\n\n```\n\n#### Part I: Question 3\n\nFit **model_forward** and **model_backward** models on **validationset** data and calculate the corresponding **RMSE** values.\n\n```{r, echo=TRUE}\n# HINT 1: Use predict () function to get the predictions. RMSE formula: sqrt(mean((Actual-Fitted)^2))\n\n# Fit model_forward on validationset and calculate RMSE\npred_forward <- predict(model_forward, newdata = validationset)\n\n#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.\nRMSE_forward <- sqrt(mean((pred_forward - validationset$Y)^2))\n\nsummary(pred_forward)\nprint(RMSE_forward)\n\n# HINT 2: An easier way would be to use rmse() function in Metrics package\n# Enter your code below\n\n\n\n\n\n```\n\n```{r}\n# Fit model_backward on validationset and calculate RMSE\npred_backward <- predict(model_backward, newdata = validationset)\n\n#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.\nRMSE_backward <- sqrt(mean((pred_backward - validationset$Y)^2))\n\nsummary(pred_backward)\nprint(RMSE_backward)\n\n\n```\n\n## KNN Regression\n\nIn this part, you will be totally blind to **testset** (You can't use testset in part II).\n\nIn Part II, by using the **caret** package in R, your task is to fit the following five models to the **first400** dataset by using K-nearest neighbors regression (KNN regression) method to find the right value of k for each model.\n\n-   **model1**: $Y=\\beta_{0}+\\beta_{1}MRP+\\beta_{2}SMB+\\epsilon$\n\n-   **model5**: $Y=\\beta_{0}+\\beta_{1}MRP+\\beta_{2}SMB+\\beta_{3}HML+\\beta_{4}MOM+\\beta_{5}BAB+\\beta_{6}QMJ+\\epsilon$\n\n#### PART II Question 4\n\nUse the **train** function in **caret** package, use knn to train **model1** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function and name your model as **knn_model1**.\n\nWhat is the average RMSE at optimal k value?\n\n```{r, echo=TRUE}\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\n#preProcess=c('center', 'scale') \n\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\npreproc <- list(preProcess = c('center', 'scale'))\n\n\n# train knn model with first400 data\nknn_model1 <- train(Y ~ MRP + SMB, data = first400, method = \"knn\", preProcess = c('center', 'scale'),\n                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), trControl = trainControl(method = \"cv\", number = 10))\n\n\n# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search\n#tuneGrid = expand.grid(k = seq(1, 50, by = 2))\n\n\n# HINT: Use trControl = trainControl(method = \"CV\", number = 10) for 10-fold cros validation\ntrControl <- trainControl(method = \"cv\", number = 10)\n\n\n\n# HINT: knn_model1$results will produce the average results\noptimal_k <- knn_model1$bestTune$k\nRMSE <- knn_model1$results[knn_model1$results$k == optimal_k, \"RMSE\"]\nRMSE\n\n\n# Enter your code below\n\n\nset.seed(2022)\n\n\n\n\n```\n\n#### PART II Question 5\n\nUse the **train** function in **caret** package, use knn to train **model5** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function. Call your model **knn_model5**.\n\nWhat is the optimal k value?\n\n```{r, echo=TRUE}\n# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\n# HINT: Use trControl = trainControl(method = \"CV\", number = 10) for 10-fold cros validation\n# HINT: knn_model5$bestTune will produce the average results\n# Enter your code below\n\n\nset.seed(2022)\n\n# Define preprocessing method\npreproc <- c('center', 'scale')\n\n# Train KNN model for model5\nknn_model5 <- train(Y ~ MRP + SMB + HML + MOM + BAB + QMJ, data = first400, \n                    method = \"knn\", preProcess = preproc, \n                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), \n                    trControl = trainControl(method = \"cv\", number = 10))\n\n# Find optimal k value\noptimal_k <- knn_model5$bestTune$k\noptimal_k\n\n\n```\n\n#### PART II Question 6\n\nUse **knn_model5** to predict Y in **testset** data and name your predictions as **knn_model5_predict**. What is the **RMSE** value in testset based on knn_model5_predict?\n\n```{r, echo=TRUE}\n# HINT: use predict function\n# HINT: you canuse rmse() function in Metrics package\n# Enter your code below\n\n# predict Y in testset using knn_model5\nknn_model5_predict <- predict(knn_model5, newdata = testset)\n\n# calculate RMSE value in testset\nRMSE <- sqrt(mean((testset$Y - knn_model5_predict)^2))\n\n# print RMSE value\nRMSE\n```\n\n#### PART III Question 7\n\nIf we define best model as the one with lowest RMSE value in \\*\\***testset**\\*\\*, which of the following is your best model?\n\n```{r, echo=TRUE}\n# HINT: use predict() function to get the predictions on testset\n# HINT: you canuse rmse() function in Metrics package to calculate RMSE values\n# Enter your code below\n\n# Predictions for knn_model1\nknn_model1_predict <- predict(knn_model1, newdata = testset)\nrmse(testset$Y, knn_model1_predict)\n\n# Predictions for knn_model5\nknn_model5_predict <- predict(knn_model5, newdata = testset)\nrmse(testset$Y, knn_model5_predict)\n\n# Predictions for model_forward\nmodel_forward_predict <- predict(model_forward, newdata = testset)\nrmse(testset$Y,model_forward_predict )\n\n# Predictions for model_backward\nmodel_backward_predict <- predict(model_backward, newdata = testset)\nrmse(testset$Y,model_backward_predict )\n \n#library(Metrics)\nRMSE_knn1 <- rmse(predict(knn_model1, newdata = testset), testset$Y)\nRMSE_knn5 <- rmse(knn_model5_predict, testset$Y)\nRMSE_backward <- rmse(model_backward_predict, testset$Y )\nRMSE_forward <- rmse(model_forward_predict, testset$Y )\n\n# print(paste(\"The value of my object is\", my_object))\nprint(paste(\"The value of RMSE_knn1 = \", RMSE_knn1))\nprint(paste(\"The value of RMSE_knn5 = \", RMSE_knn5))\nprint(paste(\"The value of RMSE_backward = \", RMSE_backward))\nprint(paste(\"The value of RMSE_forward = \", RMSE_forward))\n\n```\n\n#### Question 8\n\nClick on Render icon on to convert this file into HTML format before submitting in Canvas\n","srcMarkdownNoYaml":"\n\n## Week 3 Assignment\n\n## Return Prediction: Brief Explanation\n\nIn Lab3, we will use the stock return data. We have monthly excess return data on a diversified portfolio along with 5 factors that are shown in the literature to have an impact on stock returns. Nobel Laureate Eugene Fama and Kenneth French originally introduced the three factors model in their 1993 Journal of Financial Economics article. Then, 2 more factors were added to the original model, called the 5-factor model. In this lab assignment, your task is to apply model selection techniques, knn, and cross-validation methods to come up with a model that will be used to estimate stock returns in new data set and the test root mean squared error (RMSE).\n\n-   **TARGET VARIABLE:** Excess return on a diversified portfolio and it is captured as return on a portfolio - risk free rate (return on Long-term US Government Bond returns).\n\n-   **5-Factors:**\n\n    1\\. SIZE:Small-cap stocks tend to outperform large-cap stocks (Size is measured by stock price \\* shares outstanding)\n\n    2\\. VALUE: Cheaper stocks (Value stocks) tend to outperform expensive (Growth) stocks (Inexpensiveness: Book Value/Market Value, Book to Market ratio, B/M)\n\n    \\- Lower the B/M, expensive the stock (Growth stocks)\n\n    \\- Higher the B/M, cheap the stock (Value Stocks)\n\n    3\\. MOMENTUM: Winners outperform losers\n\n    4\\. RISK (BETA): Lower the beta of a stock, higher the return performance\n\n    5\\. QUALITY: Higher the profitability, higher the return performance\n\n## Data Dictionary\n\n-   We have 500 observations in the original data, called Full_data. Data spans from November 1976 till June 2018.\n\n-   We divided Full_data into two sets: first400 and testset. The first 400 monthly observations were kept for training and validation purposes. Monthly data from November 1976 till February 2010 were randomly divided into two groups. You can use the trainingset to train alternative models and validationset to check your model performance.\n\n-   **testset**: The last 100 monthly observations are kept as our testing data and it spans from March 2010 till June 2018.\n\n**Target Variable**\n\n**Y**: Excess return on a portfolio= Portolio return - risk free rate (return on US Government bonds)\n\n**Factors (Predictors)**\n\n1\\. SMB to capture size\n\n2\\. HML to capture Value\n\n3\\. MOM to capture Momentum\n\n4\\. BAB to capture Risk\n\n5\\. QMJ to capture Quality\n\n6\\. MRP: A measure of average market risk premium: measures as return on a value-weighted market portfolio - risk free rate.\n\nRun the following R chunk code before working on the questions.\n\n```{r, echo=FALSE}\n# WARNING: Do not modify the codes in here. \n# Run this code before moving to the next one\n\nlibrary(PerformanceAnalytics)\nlibrary(xts)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(e1071)\nlibrary(class)\nlibrary(ggplot2)\nlibrary(Metrics)\n\n\nmy_factors <- read.csv(\"Data_RLab3.csv\") # call the data\nmy_factors$Date <- mdy(my_factors$Date) # declare the date variable\nmy_factors_sorted<- my_factors[order(my_factors$Date),] # sort by date\nAll_data <- xts(my_factors_sorted[,-1],order.by = my_factors_sorted[,1],)\nAll_data$Y<-All_data$Brk_ret-All_data$RF  # target variable\n\nFull_data<-as.data.frame(All_data) # convert to data frame\nFulldata = subset(Full_data, select = -c(RF,Brk_ret,Brk_exret,Subperiod, Mkt))# drop redundant ones\nFulldata<-Fulldata%>%\n    rename(MRP=Mkt_rf, MOM=Mom)\n\nfirst400<-Fulldata[1:400,]  # use the first 400 as training and validation set\ntestset<-Fulldata[401:500,]  # last 100 for the test set\n\nset.seed(5410)   # use this seed\n# shuffle the index for the testing data\nshuffle<-sample(nrow(first400), 0.25*nrow(first400))\n # Get the training data in training set\ntrainingset<-first400[-shuffle,]\n# Get the validation set in trainingf  data\nvalidationset<-first400[shuffle,]\n\n\n```\n\n## PART I\n\nIn this part, you will be totally blind to **testset** (You can't use **testset** in part I).\n\n#### Part I: Question 1\n\nUse the step function in stats package and run a forward stepwise regression on **trainingset** and name your model **model_forward**. If we use AIC information criteria, which variables are selected based on model_forward?\n\n```{r, echo=TRUE}\n# HINT: use step function and choose either criterion = \"AIC\" and criterion = \"BIC\"\n# Enter your code below\n\nminM <- lm(Y~ 1, data=trainingset)\nmaxM<-lm(Y~. , data=trainingset)\n\n\n#model_forward <- step(lm(Y ~ ., data = trainingset), direction = \"forward\", trace = FALSE, k = log(nrow(trainingset))) # I dont understand how this line of code is the same as the following line of code.\n\n\nmodel_forward <-stats::step(minM ,direction= \"forward\", scope=formula(maxM), criterion = \"AIC\") #is using the stepwise regression method to build a linear                          regression model.   \n\nsummary(model_forward)\n\n```\n\n```{r}\n# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.\n\n#Model_backward <-stats::step(maxM,direction=\"backward\", scope=formula(maxM),criterion =\"BIC\")\n\n#summary(Model_backward)\n```\n\n#### Part I: Question 2\n\nUse the step function in stats package and run a backward stepwise regression on **trainingset** and name your model **model_backward**. If we use **BIC** information criteria, which variables are selected based on **model_backward**?\n\n```{r, echo=TRUE}\n# HINT: use step function and choose either criterion = \"AIC\" and criterion = \"BIC\"\n# Enter your code below\n\n# performs a backward stepwise regression using the BIC criterion. It starts with the full model (maxM) and removes one predictor at a time, checking if the model fit improves according to the BIC criterion.\n\nmodel_backward <-stats::step(maxM,direction=\"backward\", scope=formula(maxM),criterion =\"BIC\")\n\nsummary(model_backward)\n\n```\n\n#### Part I: Question 3\n\nFit **model_forward** and **model_backward** models on **validationset** data and calculate the corresponding **RMSE** values.\n\n```{r, echo=TRUE}\n# HINT 1: Use predict () function to get the predictions. RMSE formula: sqrt(mean((Actual-Fitted)^2))\n\n# Fit model_forward on validationset and calculate RMSE\npred_forward <- predict(model_forward, newdata = validationset)\n\n#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.\nRMSE_forward <- sqrt(mean((pred_forward - validationset$Y)^2))\n\nsummary(pred_forward)\nprint(RMSE_forward)\n\n# HINT 2: An easier way would be to use rmse() function in Metrics package\n# Enter your code below\n\n\n\n\n\n```\n\n```{r}\n# Fit model_backward on validationset and calculate RMSE\npred_backward <- predict(model_backward, newdata = validationset)\n\n#The lower the RMSE, the better the model fits the data.The validationset$Y is referring to the target variable Y in the validation set.\nRMSE_backward <- sqrt(mean((pred_backward - validationset$Y)^2))\n\nsummary(pred_backward)\nprint(RMSE_backward)\n\n\n```\n\n## KNN Regression\n\nIn this part, you will be totally blind to **testset** (You can't use testset in part II).\n\nIn Part II, by using the **caret** package in R, your task is to fit the following five models to the **first400** dataset by using K-nearest neighbors regression (KNN regression) method to find the right value of k for each model.\n\n-   **model1**: $Y=\\beta_{0}+\\beta_{1}MRP+\\beta_{2}SMB+\\epsilon$\n\n-   **model5**: $Y=\\beta_{0}+\\beta_{1}MRP+\\beta_{2}SMB+\\beta_{3}HML+\\beta_{4}MOM+\\beta_{5}BAB+\\beta_{6}QMJ+\\epsilon$\n\n#### PART II Question 4\n\nUse the **train** function in **caret** package, use knn to train **model1** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function and name your model as **knn_model1**.\n\nWhat is the average RMSE at optimal k value?\n\n```{r, echo=TRUE}\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\n#preProcess=c('center', 'scale') \n\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\npreproc <- list(preProcess = c('center', 'scale'))\n\n\n# train knn model with first400 data\nknn_model1 <- train(Y ~ MRP + SMB, data = first400, method = \"knn\", preProcess = c('center', 'scale'),\n                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), trControl = trainControl(method = \"cv\", number = 10))\n\n\n# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search\n#tuneGrid = expand.grid(k = seq(1, 50, by = 2))\n\n\n# HINT: Use trControl = trainControl(method = \"CV\", number = 10) for 10-fold cros validation\ntrControl <- trainControl(method = \"cv\", number = 10)\n\n\n\n# HINT: knn_model1$results will produce the average results\noptimal_k <- knn_model1$bestTune$k\nRMSE <- knn_model1$results[knn_model1$results$k == optimal_k, \"RMSE\"]\nRMSE\n\n\n# Enter your code below\n\n\nset.seed(2022)\n\n\n\n\n```\n\n#### PART II Question 5\n\nUse the **train** function in **caret** package, use knn to train **model5** with **first400** data. Use 10-fold cross validation. Use the **set.seed(2022)** seed values and by using expand.grid, evaluate odd k values up to 50. Use scaled and centered data by using the preProcess function. Call your model **knn_model5**.\n\nWhat is the optimal k value?\n\n```{r, echo=TRUE}\n# HINT: Use tuneGrid = expand.grid(k = seq(1, 50, by = 2)) for odd k grid search\n# HINT: Use preProcess=c('center', 'scale') to preprocess the data\n# HINT: Use trControl = trainControl(method = \"CV\", number = 10) for 10-fold cros validation\n# HINT: knn_model5$bestTune will produce the average results\n# Enter your code below\n\n\nset.seed(2022)\n\n# Define preprocessing method\npreproc <- c('center', 'scale')\n\n# Train KNN model for model5\nknn_model5 <- train(Y ~ MRP + SMB + HML + MOM + BAB + QMJ, data = first400, \n                    method = \"knn\", preProcess = preproc, \n                    tuneGrid = expand.grid(k = seq(1, 50, by = 2)), \n                    trControl = trainControl(method = \"cv\", number = 10))\n\n# Find optimal k value\noptimal_k <- knn_model5$bestTune$k\noptimal_k\n\n\n```\n\n#### PART II Question 6\n\nUse **knn_model5** to predict Y in **testset** data and name your predictions as **knn_model5_predict**. What is the **RMSE** value in testset based on knn_model5_predict?\n\n```{r, echo=TRUE}\n# HINT: use predict function\n# HINT: you canuse rmse() function in Metrics package\n# Enter your code below\n\n# predict Y in testset using knn_model5\nknn_model5_predict <- predict(knn_model5, newdata = testset)\n\n# calculate RMSE value in testset\nRMSE <- sqrt(mean((testset$Y - knn_model5_predict)^2))\n\n# print RMSE value\nRMSE\n```\n\n#### PART III Question 7\n\nIf we define best model as the one with lowest RMSE value in \\*\\***testset**\\*\\*, which of the following is your best model?\n\n```{r, echo=TRUE}\n# HINT: use predict() function to get the predictions on testset\n# HINT: you canuse rmse() function in Metrics package to calculate RMSE values\n# Enter your code below\n\n# Predictions for knn_model1\nknn_model1_predict <- predict(knn_model1, newdata = testset)\nrmse(testset$Y, knn_model1_predict)\n\n# Predictions for knn_model5\nknn_model5_predict <- predict(knn_model5, newdata = testset)\nrmse(testset$Y, knn_model5_predict)\n\n# Predictions for model_forward\nmodel_forward_predict <- predict(model_forward, newdata = testset)\nrmse(testset$Y,model_forward_predict )\n\n# Predictions for model_backward\nmodel_backward_predict <- predict(model_backward, newdata = testset)\nrmse(testset$Y,model_backward_predict )\n \n#library(Metrics)\nRMSE_knn1 <- rmse(predict(knn_model1, newdata = testset), testset$Y)\nRMSE_knn5 <- rmse(knn_model5_predict, testset$Y)\nRMSE_backward <- rmse(model_backward_predict, testset$Y )\nRMSE_forward <- rmse(model_forward_predict, testset$Y )\n\n# print(paste(\"The value of my object is\", my_object))\nprint(paste(\"The value of RMSE_knn1 = \", RMSE_knn1))\nprint(paste(\"The value of RMSE_knn5 = \", RMSE_knn5))\nprint(paste(\"The value of RMSE_backward = \", RMSE_backward))\nprint(paste(\"The value of RMSE_forward = \", RMSE_forward))\n\n```\n\n#### Question 8\n\nClick on Render icon on to convert this file into HTML format before submitting in Canvas\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"embed-resources":true,"output-file":"Week 3 Assignment_RWi.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Danger","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.287","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Week 3 Assignment"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}