---
title: "Mini Project 2: Option 1"
author: Randy Williams, Venessa Ricketts, Adan J Salinas
format: html
editor: visual
---

```{r, echo=TRUE}
# Run this code chunk without changing it
library(dplyr)
library(caret)
library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)



```

# MINI PROJECT 2:

As a group, you consult to a bank that is interested in understanding what drives loans made by the bank to default ("bad" status). You are provided with a data set that contains 4,455 observations and 14 variables, shown below:

**Target Variable**

-   **Status:** credit status (Good=1, Bad=2)

**Predictors**

-   **Seniority** job seniority (years)

-   **Home** type of homeownership (1=rent, 2=owner, 3=priv, 4=ignore, 5=parents, 6=other)

-   **Time** time of requested loan

-   **Age** client's age

-   **Marital** marital status (1=single, 2=married, 3=widow, 4=separated, 5=divorced)

-   **Records** existence of records (1=no, 2=yes)

-   **Job** type of job(1=fixed, 2=parttime, 3=freelance, 4=others)

-   **Expenses** amount of expenses

-   **Income** amount of income

-   **Assets** amount of assets

-   **Debt** amount of debt

-   **Amount** amount requested of loan

-   **Price** price of good

This project is composed of three parts:

-   Part 1\[20 points\]: Data Prep

-   Part 2 \[45 points\]: Prediction with Decision Trees

-   Part 3\[35 points\]: Prediction with Logistic Regression

## Read the data in R

```{r, echo=TRUE}
# Run this code chunk without changing it
creditdata<-read.csv("MP2_data_option1.csv", header = TRUE)
creditdata
```

## PART 1: Data Preperation

Your first task is to clean the data as instructed below. Normally, you would need to do a through quality check on the data but for this group project, we will focus more on the modelling part. In real life, before modelling your data, you would need to take a deeper look at the shape and structure of your data set. Things like identifying errors, checking the distributions of your variables, checking for need for data transformation, should be always in your checklist before modeling your data.

#### Task 1A: Data preparation

There were some data entry errors:

-   **Status** variable was coded 0 for certain individuals. Drop rows from **creditdata** when **Status** variable takes the value of 0.

-   **Marital** variable was coded 0 for certain individuals. Drop rows from **creditdata** when **Marital** variable takes the value of 0.

-   **Job** variable was coded 0 for certain individuals. Drop rows from **creditdata** when **Job** variable takes the value of 0.

-   For some variables, the missing values were coded with 99999999 to indicate that the observation is missing. Drop rows from **creditdata** when **Income, Assets,** or **Debt** variable takes the value of 99999999. You can use subset function for this task.

-   Declare the following variables as factor: **Status, Home, Marital, Records**, and **Job**.

-   Label **Status** variable as "*Good" and "Bad"*

-   If you end up with 4375 rows, then you are on the right track.

```{r, echo=TRUE}
# Insert your codes for Task 1A  below
# Drop rows from creditdata when Status variable takes the value of 0.
creditdata1 <- subset(creditdata, Status != 0) 
creditdata1 
```

```{r, echo=TRUE}
# Drop rows from creditdata when Marital variable takes the value of 0.
creditdata2 <- subset(creditdata1, Marital != 0) 
creditdata2
```

```{r, echo=TRUE}
# Drop rows from creditdata when Job variable takes the value of 0.
creditdata3 <- subset(creditdata2, Job != 0)
creditdata3
```

```{r, echo=TRUE}
# Drop rows from creditdata when Income, Assets, or Debt variable takes the value of 99999999.

creditdata4 <- subset(creditdata3, Income != 99999999 & Assets != 99999999 & Debt != 99999999)
creditdata4
```

```{r, echo=TRUE}
# Declare the following variables as factor: Status, Home, Marital, Records, and Job

creditdata4$Status <- factor(creditdata4$Status)
creditdata4$Home <- factor(creditdata4$Home)
creditdata4$Marital <- factor(creditdata4$Marital)
creditdata4$Records <- factor(creditdata4$Records)
creditdata4$Job <- factor(creditdata4$Job)

```

```{r, echo=TRUE}
# Label Status variable as "Good" and "Bad"

creditdata4$Status <- factor(creditdata4$Status, levels = c(1, 2), labels = c("Good", "Bad"))

creditdata4
```

#### Task 1B: Split your data

By using **createDataPartition** function in **caret** package, split the **creditdata** by holding 75% of the data in **train_data**, and the rest in **test_data**. Use **set.seed(5410)** when you do the split .

```{r, echo=TRUE}
# Insert your codes for Task 1B  below
set.seed(5410) 

index <- createDataPartition(creditdata4$Status, p = 0.75, list = FALSE)
train_data <- creditdata4[index, ]
test_data <- creditdata4[-index, ]

train_data
test_data
```

## Part 2: Classification Tree and Ensemble Model

### Task 2A: Training with Classication Tree

First, you will use a classification tree to predict **Status** in **train_data** with all the predictors in our data set. Use **rpart** function in **rpart** package to build a decision tree to estimate **Status** by using the **train_data** and name your model as **model_tree**. Since we construct classification tree, you need to use *method="class"* in **rpart** function.

Use the following parameters in **model_tree** (Hint: use **rpart.control** in **rpart** function).

-   use 10-fold cross validation (**xval=10**)

-   use complexity parameter of 0.001 (**cp=0.001**)

-   use at least 3 observations in each terminal node (**minbucket=3**)

-   Based on **model_tree** results, which three variables contribute most to classify **Status** in the **train_data**?

```{r, echo=TRUE}
# Insert your codes for Task 2A  below

model_tree <- rpart(Status ~ ., data = train_data, method = "class",
                    control = rpart.control(cp = 0.001, minbucket = 3, xval = 10))
plot(model_tree)
```

```{r}
#Here is the code you can use to extract the variable importance measures from the model_tree object:

var_importance <- varImp(model_tree, scale = FALSE)
var_importance

# To extract the top three most important variables, you can use the following code:

top_vars <- sort(var_importance[, "Overall"], decreasing = TRUE)[1:3]
top_vars
```

### TASK 2B: Predict Status in the test data

-   By using **model_tree**, predict **Status** labels in **test_data** and store them as **predict_model_tree**. You can use **predict()** function for this task and select **type="class"** to retrieve labels. We define Good credit status as *positive class* (when Status=1) and Bad credit status as *Negative class* (when Status=2).

-   Now, we need the performance measures to compare **model_tree** with the models you will create in the following sections. By using the actual and predicted Status labels in **test_data**, do the followings:

1.  Calculate accuracy and name it as accuracy_model_tree

2.  Calculate precision and name it as precision_model_tree

3.  Calculate sensitivity and name it as sensitivity_model_tree

4.  Calculate specificity and name it as specificity_model_tree

```{r, echo=TRUE}
# Insert your codes for Task 2B  below
# To use the model_tree object to predict Status labels in test_data and store them as predict_model_tree, you can use the predict() function in R with type="class".

predict_model_tree <- predict(model_tree, newdata = test_data, type = "class")
predict_model_tree
```

```{r, echo=TRUE}
# Calculate accuracy and name it as accuracy_model_tree

actual_labels <- test_data$Status
accuracy_model_tree <- mean(predict_model_tree == actual_labels)
accuracy_model_tree

```

```{r, echo=TRUE}
# Calculate precision and name it as precision_model_tree

conf_matrix <- confusionMatrix(predict_model_tree, actual_labels)
conf_matrix
precision_model_tree <- conf_matrix$byClass["Pos Pred Value"]
precision_model_tree

```

```{r, echo=TRUE}
# Calculate sensitivity and name it as sensitivity_model_tree

conf_matrix <- confusionMatrix(predict_model_tree, actual_labels)
sensitivity_model_tree <- conf_matrix$byClass["Sensitivity"]
sensitivity_model_tree
```

```{r, echo=TRUE}
# Calculate specificity and name it as specificity_model_tree

conf_matrix <- confusionMatrix(predict_model_tree, actual_labels)
specificity_model_tree <- conf_matrix$byClass["Specificity"]
specificity_model_tree


```

## Training with Random Forest Model

In this task, we will see if random forest model can help us to improve our prediction. In Random forest, many different trees are fitted to random subsets of the data via bootstrapping, then tree averages/majorities are used in final classification. We will use **ranger** function but since we want to go beyond out-of-bag error rate performance measure and want to get a better sense of the model performance, we will call **ranger** within **train()** function in **caret** package (**method="ranger"**). This way, we can tune the parameters of the model.

In ensemble models such as Random forest, Out of Bag and Cross-Validation are the two resampling solutions for tuning the model. With **trainControl()** function, we can modify the default selections. In this project, we will use 10-fold cross-validation **(trainControl(method="cv",number=10)).**

In this project, search through **mtry** values 2,5,7,9,11, and 13 and use "**gini**" as the split rule (**splitrule**). For minimum node size, check values from 1 to 5 (**min.node.size**).

## TASK 2C: Training with Random Forest Model

By using the **train**() function in **caret** package, use random forest model with **ranger** method to estimate **Status** in **train_data** with the the tuning parameters provided above. Name your model as **model_rf** and use **set.seed(5410)** for reproducible findings.

```{r, echo=TRUE}
# Insert your codes for Task 2C  below

# In this project, search through mtry values 2,5,7,9,11, and 13 and use "gini" as the split rule (splitrule). For minimum node size, check values from 1 to 5 (min.node.size).

set.seed(5410) # set seed for reproducibility
tune_grid <- expand.grid(mtry = c(2, 5, 7, 9, 11, 13), 
                         splitrule = "gini",
                         min.node.size = 1:5)
model_rf <- train(Status ~ ., data = train_data, method = "ranger",
                   tuneGrid = tune_grid,
                   trControl = trainControl(method = "cv", number = 10),
                   importance = "permutation")

```

-   What is the highest accuracy measure in **model_rf**? Which specific parameters (mtry and min.node.size) give us the highest accuracy?

```{r, echo=TRUE}
#What is the highest accuracy measure in model_rf? Which specific parameters (mtry and min.node.size) give us the highest accuracy?

summary(model_rf)


```

ENTER YOUR ANSWER IN HERE!

```{r, echo=TRUE}
# What is the highest accuracy measure in model_rf? Which specific parameters (mtry and min.node.size) give us the highest accuracy?

best_acc <- max(model_rf$results$Accuracy)
best_acc
best_params <- subset(model_rf$results, Accuracy == best_acc, 
                      select = c(mtry, min.node.size))
best_params



```

### TASK 2D: Prediction with Random Forest Model

Based on the best tuned parameters in **model_rf**, predict **Status** labels in **test_data** and store your predictions as **predict_model_rf**.

```{r, echo=TRUE}
# Insert your codes for Task 2D  below

#predict_model_rf <- predict(model_rf$finalModel, newdata = test_data, type = "class")

#predict_model_rf <- predict(model_rf$finalModel, data = test_data, type = "class")

predict_model_rf <- predict(model_rf, newdata=test_data, type = "raw")
summary(predict_model_rf)
```

Print the **Confusion Matrix** and comment on your findings. Which model (**model_rf** or **model_tree**) does a perfect job to predict **Status** in **test_data** based on **Accuracy** **ratio**?

```{r, echo=TRUE}

actual_labels <- factor(test_data$Status, levels = levels(train_data$Status))
conf_matrix_rf <- confusionMatrix(predict_model_rf, actual_labels)
conf_matrix_rf
```

## TASK 2E: In search of a better model?

Now, your task is to modify **model_rf** with different tuning parameters to see if you can get a higher accuracy ratio for test data. Name your revised model as **model_rf_best** and use **set.seed(5410)** for reproducible findings.

```{r, echo=TRUE}
# Insert your codes for Task 2E  below
set.seed(5410) # set seed for reproducibility
tune_grid <- expand.grid(mtry = c(2, 3, 8, 11, 15, 19), 
                         splitrule = "gini",
                         min.node.size = 1:7)
model_rf_best <- train(Status ~ ., data = train_data, method = "ranger",
                   tuneGrid = tune_grid,
                   trControl = trainControl(method = "oob", number = 7),
                   importance = "permutation")


```

```{r, echo=TRUE}
#Continuing task 2E
predict_model_rf <- predict(model_rf, newdata=test_data, type = "raw")
summary(predict_model_rf)
actual_labels <- factor(test_data$Status, levels = levels(train_data$Status))
conf_matrix_rf <- confusionMatrix(predict_model_rf, actual_labels)
conf_matrix_rf

#This model is worse!
```

```{r,echo=TRUE}
#Attempt #2 for Task 2e
set.seed(5410) # set seed for reproducibility
tune_grid <- expand.grid(mtry = c(1, 2, 4, 5, 7, 9), 
                         splitrule = "gini",
                         min.node.size = 1:7)
model_rf <- train(Status ~ ., data = train_data, method = "ranger",
                   tuneGrid = tune_grid,
                   trControl = trainControl(method = "cv", number = 15),
                   importance = "permutation")

```

```{r, echo=TRUE}
#Part of the 2nd attempt
predict_model_rf <- predict(model_rf, newdata=test_data, type = "raw")
summary(predict_model_rf)
actual_labels <- factor(test_data$Status, levels = levels(train_data$Status))
conf_matrix_rf <- confusionMatrix(predict_model_rf, actual_labels)
conf_matrix_rf

#This one is a bit better
```

## Part 3: Logistic Regression

Use the **train_data** data to perform logistic regression for the following two models: The first one uses only three predictors, the second one is the multiple logistic regression with all predictors. Given that $P(Y)$ stands for the probability of being "*Good Status*" (*Status="Good" or Y=1*), the two logistic models are as follows:

-   $Logistic1 = log(\frac{P(Y)}{1-P(Y)})=\beta_{0}+\beta_{1}Income+\beta_{2}Price+\beta_{3}Amount$

-   $Logistic2 = log(\frac{P(Y)}{1-P(Y)})=\beta_{0}+\beta_{1}Seniority+\beta_{2}Home+\beta_{3}Time+\beta_{4}Age+\beta_{5}Marital+\beta_{6}Records+\beta_{7}Job+\\~~~~~~~~~~~~~~~~~~~~\beta_{8}Expenses +\beta_{9}Income+\beta_{10}Assets+\beta_{11}Debt+\beta_{12}Amount+\beta_{13}Price$

The left side of the equation above is called logged odds or logit.

## Task 3A: Accessing Model Accuracy: Confusion matrix

Use **train** function in **caret** package and by using the **train_data** data, fit 10-fold cross validated logistic regression for models **Logistic1** and **Logistic2** . Set the seed function as **set.seed(5410)**. By using the **confusionMatrix()** function in **caret** package, calculate the confusion matrix for each model. What the confusion matrix is telling you about the types of mistakes made by logistic regression?

ENTER YOUR ANSWER IN HERE!

```{r, echo=TRUE}
# Insert your codes for Task 3A  below

set.seed(5410)
Logistic1 <- train(Status ~ Income + Price + Amount,
                   data=train_data,
                   method = "glm",
                  # tuneGrid = tune_grid,
                   trControl = trainControl(method = "cv", number = 10),
                   )

Logistic2 <- train(Status ~ Seniority + Home + Time + Age + Marital + Records + Job + Expenses + Income + Assets + Debt + Amount + Price,
                   data=train_data,
                   method = "glm",
                   trControl = trainControl(method = "cv", number = 10),
                   )

```

## Task 3B: Predict Status with Logistic Reression

-   By using **Logistic1**, predict **Status** labels in **test_data** and store them as **predict_Logistic1**. We define Good credit status as positive class (when Status=1) and Bad credit status as Negative class (when Status=2).

-   By using **Logistic2**, predict **Status** labels in **test_data** and store them as **predict_Logistic2**.

-   By using the actual and predicted **Status** labels in **test_data**, print the following performance measures for **Logistic1** and **Logistic2**:

-   Accuracy

-   Sensitivity

-   Specificity

```{r echo=TRUE}
# Insert your codes for Task 3B  below
predict_Logistic1 <- predict(Logistic1, newdata=test_data, type="raw")
summary(predict_Logistic1)
predict_Logistic2 <- predict(Logistic2, newdata=test_data, type="raw")
summary(predict_Logistic2)
```

```{r,echo=TRUE}
#Accuracy, Sensitivity, and Specificity

accuracylogistic1 <- mean(predict_Logistic1== actual_labels)
accuracylogistic1
accuracylogistic2 <- mean(predict_Logistic2== actual_labels)
accuracylogistic2


# Calculate precision and name it as precision_model_tree

confmatrixLog1 <- confusionMatrix(predict_Logistic1, actual_labels)
confmatrixLog1
confmatrixLog2 <- confusionMatrix(predict_Logistic2, actual_labels)
confmatrixLog2
precisionlog1 <- confmatrixLog1$byClass["Pos Pred Value"]
precisionlog1
precisionlog2 <- confmatrixLog2$byClass["Pos Pred Value"]
precisionlog2

# Calculate specificity and name it as specificity_model_tree

confmatlog1 <- confusionMatrix(predict_Logistic1, actual_labels)
specificity_log1 <- confmatlog1$byClass["Specificity"]
specificity_log1
confmatlog2 <- confusionMatrix(predict_Logistic2, actual_labels)
specificity_log2 <- confmatlog2$byClass["Specificity"]
specificity_log2
```

## Task 3C: Model Selection

Based on your findings in Sections 1, 2 and 3, which model performs best in **test_data** by using the **Accuracy** measure?

ENTER YOUR ANSWER IN HERE!

From an Accuracy standpoint the Logistic2 model outperforms the others at 82% accracy. This is about 8% better than the worst model, Logistic1 model. The others were close, but not as accurate as Logistic2 model.
